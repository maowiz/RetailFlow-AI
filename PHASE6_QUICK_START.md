# Phase 6 Groq Insights - Quick Start Guide

## üöÄ Quick Setup (5 minutes)

### 1. Installation (Already Complete!)
```powershell
pip install groq python-dotenv  # ‚úÖ Already installed
```

### 2. API Key Configuration
Your API key is already configured in `.env`:
```
GROQ_API_KEY=gsk_YOUR_KEY_HERE
```

### 3. Import and Use
```python
from src.insights import GroqInsightsEngine, SalesAnomalyDetector

# Initialize
engine = GroqInsightsEngine()

# Ready to use!
```

---

## üìñ Common Use Cases

### Use Case 1: Generate Weekly Report for Leadership

```python
import json
from src.insights import GroqInsightsEngine
from datetime import datetime

# Load your data
with open('data/output/financial_impact.json') as f:
    financial_data = json.load(f)

# Initialize engine
engine = GroqInsightsEngine()

# Generate report
report = engine.generate_weekly_report(
    forecast_data={
        'model': 'Ensemble',
        'accuracy': '98.78%',
        'stores_covered': 54
    },
    inventory_data={
        'total_value': financial_data['executive_summary']['inventory_health'],
        'turnover': financial_data['executive_summary']['inventory_health']['avg_inventory_turnover']
    },
    financial_data=financial_data['savings'],
    week_date=datetime.now().strftime('%Y-%m-%d')
)

print(report)
# Copy & paste into email to CFO/VP Operations
```

### Use Case 2: Detect and Alert on Anomalies

```python
import pandas as pd
from src.insights import SalesAnomalyDetector, GroqInsightsEngine

# Load forecast data
df = pd.read_parquet('data/output/forecasts.parquet')

# Detect anomalies
detector = SalesAnomalyDetector(
    z_threshold=3.0,      # 3 standard deviations
    pct_threshold=0.5     # 50% deviation threshold
)

anomalies = detector.detect_forecast_anomalies(
    df,
    actual_col='sales',
    forecast_col='forecast_ensemble',
    segment_cols=['store_nbr', 'family']
)

# Get summary
summary = detector.get_anomaly_summary(anomalies)

print(f"Total Anomalies: {summary['total']}")
print(f"Spikes: {summary['spikes']}")
print(f"Drops: {summary['drops']}")

# Get AI explanation for top anomaly
if summary['total'] > 0:
    engine = GroqInsightsEngine()
    top = anomalies.iloc[0]
    
    explanation = engine.explain_anomaly(
        store=str(top['store_nbr']),
        category=str(top['family']),
        date=str(top['date']),
        expected_sales=float(top['forecast_ensemble']),
        actual_sales=float(top['sales'])
    )
    
    print("\nTop Anomaly Explanation:")
    print(explanation)
```

### Use Case 3: Interactive Q&A Dashboard

```python
from src.insights import GroqInsightsEngine
import json

# Load context data
with open('data/output/financial_impact.json') as f:
    data = json.load(f)

engine = GroqInsightsEngine()

# Example questions from stakeholders
questions = [
    "What are the top 3 stores by revenue?",
    "How much working capital did we free up?",
    "What is our inventory turnover rate?",
    "Which categories have the highest stockout risk?"
]

for question in questions:
    answer = engine.answer_stakeholder_question(
        question=question,
        data_context={
            'savings': data['savings'],
            'inventory': data['executive_summary']['inventory_health'],
            'risks': data['executive_summary']['risk_summary']
        }
    )
    
    print(f"\nQ: {question}")
    print(f"A: {answer}")
    print("-" * 60)
```

### Use Case 4: Automated Email Reports

```python
from src.insights import GroqInsightsEngine
import json
import smtplib
from email.mime.text import MIMEText
from datetime import datetime

# Load data
with open('data/output/financial_impact.json') as f:
    financial_data = json.load(f)

# Generate insight
engine = GroqInsightsEngine()
insights = engine.generate_forecast_insights(
    forecast_summary={
        'total_stores': 54,
        'forecast_period': '16 days',
        'model': 'Ensemble (XGBoost + RF + Prophet)'
    },
    model_comparison=financial_data['comparison']
)

# Format email
email_body = f"""
Good morning,

Here is your weekly AI forecasting summary for {datetime.now().strftime('%B %d, %Y')}:

{insights}

---
This report was automatically generated by the AI Forecasting System.
For questions, contact the Data Science team.

Best regards,
Supply Chain Analytics
"""

# Send email (configure your SMTP settings)
# msg = MIMEText(email_body)
# msg['Subject'] = f'Weekly Forecast Insights - {datetime.now().strftime("%Y-%m-%d")}'
# msg['From'] = 'analytics@company.com'
# msg['To'] = 'cfo@company.com'

# with smtplib.SMTP('smtp.gmail.com', 587) as server:
#     server.starttls()
#     server.login('your_email', 'your_password')
#     server.send_message(msg)

print("Email would be sent with this content:")
print(email_body)
```

---

## ‚öôÔ∏è Configuration Options

### Customize LLM Parameters

```python
engine = GroqInsightsEngine(
    model="llama-3.3-70b-versatile",  # Default: fastest model
    temperature=0.3,                   # 0.0 = deterministic, 1.0 = creative
    max_tokens=2000                    # Max response length
)
```

### Adjust Anomaly Detection Sensitivity

```python
# More sensitive (catch more anomalies)
detector = SalesAnomalyDetector(
    z_threshold=2.5,      # Lower = more sensitive
    pct_threshold=0.3     # Lower = more sensitive
)

# Less sensitive (only critical anomalies)
detector = SalesAnomalyDetector(
    z_threshold=4.0,      # Higher = less sensitive
    pct_threshold=0.7     # Higher = less sensitive
)
```

---

## üîç Troubleshooting

### Issue: "Groq client not initialized"
**Solution**: Check your `.env` file exists and contains valid API key

```powershell
# Verify .env file
cat .env

# Should show:
# GROQ_API_KEY=gsk_...
```

### Issue: "AI insight generation failed"
**Possible causes**:
1. Internet connection issue
2. API key invalid/expired
3. Rate limit reached (free tier: 30 requests/minute)

**Solution**: 
```python
# The engine provides fallback responses automatically
# Check logs for specific error:
import logging
logging.basicConfig(level=logging.INFO)

engine = GroqInsightsEngine()
# Logs will show: "Groq call successful | Tokens: 1234" or error details
```

### Issue: "No anomalies detected"
**This is normal!** If your forecasts are highly accurate, few anomalies will be detected.

**To verify it's working**:
```python
# Intentionally create test anomaly
test_df = df.copy()
test_df.loc[0, 'sales'] = test_df.loc[0, 'forecast_ensemble'] * 3  # 3x spike

anomalies = detector.detect_forecast_anomalies(test_df, ...)
# Should now detect at least 1 anomaly
```

---

## üìä Token Usage & Costs

Groq is **FREE** for reasonable usage:
- **Free Tier**: 30 requests/minute, 14,400 tokens/minute
- **Your typical usage**: ~800-1500 tokens per insight
- **Cost**: $0 (free tier is generous)

**Monitor usage**:
```python
# Enable logging to see token counts
import logging
logging.basicConfig(level=logging.INFO)

engine = GroqInsightsEngine()
insights = engine.generate_forecast_insights(...)
# Output: "Groq call successful | Tokens: 1234"
```

---

## üéØ Best Practices

### 1. Cache Insights for Repeated Queries
```python
import json
from datetime import datetime

# Generate once
insights = engine.generate_forecast_insights(...)

# Cache to file
cache = {
    'generated_at': datetime.now().isoformat(),
    'insights': insights
}

with open('cache/forecast_insights.json', 'w') as f:
    json.dump(cache, f)

# Reuse cached version for 24 hours
```

### 2. Batch Process Anomalies
```python
# Instead of explaining every anomaly individually:
top_10 = anomalies.head(10)  # Only top 10 most severe

for idx, anomaly in top_10.iterrows():
    explanation = engine.explain_anomaly(...)
    # Save to database or report
```

### 3. Use Specific Data Context
```python
# ‚ùå Bad: Too much data
answer = engine.answer_stakeholder_question(
    question="What's our savings?",
    data_context=entire_database  # Wasteful
)

# ‚úÖ Good: Targeted data
answer = engine.answer_stakeholder_question(
    question="What's our savings?",
    data_context={
        'annual_savings': 35_932_003_490,
        'monthly_savings': 2_994_333_624
    }
)
```

---

## üß™ Quick Test Script

Run this to verify everything works:

```python
# quick_test.py
from src.insights import GroqInsightsEngine

engine = GroqInsightsEngine()

if engine.client:
    print("‚úÖ Groq connected!")
    
    answer = engine.answer_stakeholder_question(
        question="Hello, can you hear me?",
        data_context={'test': 'data'}
    )
    
    print(f"\nResponse: {answer}")
    print("\n‚úÖ Phase 6 is working perfectly!")
else:
    print("‚ùå Check your API key in .env file")
```

```powershell
python quick_test.py
```

---

## üìö Reference

### All Available Methods

**GroqInsightsEngine:**
1. `generate_forecast_insights()` - Model performance analysis
2. `generate_inventory_insights()` - Savings & optimization summary  
3. `explain_anomaly()` - Single anomaly explanation
4. `answer_stakeholder_question()` - Free-form Q&A
5. `generate_weekly_report()` - Comprehensive weekly summary

**SalesAnomalyDetector:**
1. `detect_forecast_anomalies()` - Returns DataFrame of anomalies
2. `get_anomaly_summary()` - Returns dict with statistics

---

## üéâ You're Ready!

Phase 6 is fully operational. You can now:
- ‚úÖ Generate executive summaries
- ‚úÖ Detect and explain anomalies
- ‚úÖ Answer stakeholder questions
- ‚úÖ Automate weekly reports

**Next**: Integrate into your Streamlit dashboard or build custom workflows!

For full details, see [walkthrough.md](walkthrough.md)
